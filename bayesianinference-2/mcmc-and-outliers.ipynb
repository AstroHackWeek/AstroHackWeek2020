{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data set from  arxiv:1008.4686, table 1\n",
    "data = np.array([[201,592,61],[244,401,25],[47 ,583,38],[287,402,15],[203,495,21],[58 ,173,15],[210,479,27],\n",
    "                 [202,504,14],[198,510,30],[158,416,16],[165,393,14],[201,442,25],[157,317,52],[131,311,16],\n",
    "                 [166,400,34],[160,337,31],[186,423,42],[125,334,26],[218,533,16],[146,344,22],]).astype(float)\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "yerr = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt='o', color='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example dataset with \"outliers\" -- data points that just don't seem to fit the trend.\n",
    "\n",
    "Outliers might arise due to \"glitches\" in your observational hardware (or external interference like cosmic rays in astronomical imaging surveys), or might be due to problems with the sample of objects you're plotting -- maybe not all these stars are actually part of the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a linear model to this.  In fact, the model will still be confident about what the best-fit parameters are, even if the fit is subjectively terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_one(params, x, y, yerr):\n",
    "    b,m = params\n",
    "    ypred = \n",
    "    chi = \n",
    "    loglike = \n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper function because we're calling *minimize* but we want to maximize log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_ll_one(*X):\n",
    "    return -log_likelihood_one(*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = scipy.optimize.minimize(neg_ll_one, [0.,0.], args=(x, y, yerr))\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt='o', color='k');\n",
    "ax = plt.axis()\n",
    "# Draw a sampling of B,M parameter values that are consistent with the fit,\n",
    "# using the estimated inverse-Hessian matrix (parameter covariance)\n",
    "BM = scipy.stats.multivariate_normal.rvs(mean=R.x, cov=R.hess_inv, size=20)\n",
    "xx = np.array(ax[:2])\n",
    "for b,m in BM:\n",
    "    plt.plot(xx, b + xx*m, 'b-', alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this fit make you happy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also plot the ellipse showing the constraints in B,M space by manipulating hess_inv:\n",
    "U,s,V = np.linalg.svd(R.hess_inv)\n",
    "S = np.dot(U, np.diag(np.sqrt(s)))\n",
    "th = np.linspace(0,2.*np.pi,200)\n",
    "xy = np.vstack((np.sin(th), np.cos(th)))\n",
    "dbm = np.dot(S, xy).T\n",
    "plt.plot(R.x[0] + dbm[:,0], R.x[1] + dbm[:,1], 'r-')\n",
    "plt.xlabel('B')\n",
    "plt.ylabel('M')\n",
    "plt.title('Parameter constraints');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at how to extend our probabilistic data model to handle outliers,\n",
    "let's first look at the Markov Chain Monte Carlo family of algorithms.\n",
    "\n",
    "First, let's unpack the name.\n",
    "\n",
    "- Markov -- samples are generated by *jumping* from one point in the parameter space to another, in a way that depends only on the current point in the space\n",
    "- Chain -- a list of samples\n",
    "- Monte Carlo -- a casino in Monaco; evokes the randomness used in this algorithm\n",
    "\n",
    "The MCMC algorithm moves a \"particle\" or \"sample\" or \"walker\" randomly around the particle space,\n",
    "by first *proposing* a move, and then using the relative likelihoods of the current and proposed\n",
    "positions to decide whether to *accept* or *reject* the move.\n",
    "\n",
    "If you want to add *MOAR WORDS* to the name, you can call this specific algorithm *Metropolis--Hastings* Markov Chain Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_one(logprob, p0, stepsizes, steps, logprob_args):\n",
    "    '''\n",
    "    * logprob: a function that returns the log-probability at a given value of parameters.\n",
    "        lnp = logprob(p, *logprob_args)\n",
    "    * p0: initial position in parameter space (np array, size P)\n",
    "    * stepsizes: (np array, size P): size of Gaussian jumps in parameter space\n",
    "    * step: int: number of MCMC steps to take\n",
    "    * logprob_args: extra arguments passed to the *logprob* function\n",
    "    \n",
    "    Returns  (chain, logprobs, naccept)\n",
    "    * chain: size Nsteps x P, MCMC samples\n",
    "    * logprobs: size Nsteps, logprobs at *chain* positions\n",
    "    * naccept: int: number of MCMC proposed jumps that were accepted\n",
    "    '''\n",
    "    # Compute log-probability at initial parameters p0\n",
    "\n",
    "    # initialize output arrays\n",
    "\n",
    "    # main MCMC loop\n",
    "    for i in range(steps):\n",
    "        # Propose to jump to a new spot in parameter space.\n",
    "        # Depends only on p: hence Markov\n",
    "\n",
    "        # Compute logprob at new spot\n",
    "\n",
    "        # Our proposal distribution is symmetric (equal prob of proposing the reverse jump)\n",
    "        # so we need only compare the ratio of lnp_new to current lnp.\n",
    "        ratio = \n",
    "        # We draw a UNIFORM random number and *accept* if it is less than the ratio.\n",
    "        # (Note that this means we *always* accept if ratio>1!)\n",
    "        # Monte Carlo!\n",
    "        \n",
    "        urand = np.random.uniform()\n",
    "        if urand < ratio:\n",
    "            # Accept jump!\n",
    "        else:\n",
    "            # Reject jump!\n",
    "            pass\n",
    "        # Crucial: we save the current *p* regardless of whether we accepted the jump or not --\n",
    "        # so for rejected jumps, we have duplicate entries in the chain!\n",
    "\n",
    "        return chain, logprobs, naccept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial position in parameter space\n",
    "p0 = \n",
    "# \"Jump\" sizes\n",
    "stepsizes = \n",
    "# Number of MCMC steps\n",
    "steps = 5000\n",
    "chain, logprobs, naccept = mcmc_one(log_likelihood_one, p0, stepsizes, steps, (x, y, yerr))\n",
    "naccept/steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,0])\n",
    "plt.xlabel('MCMC sample number')\n",
    "plt.ylabel('Param B');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,1])\n",
    "plt.xlabel('MCMC sample number')\n",
    "plt.ylabel('Param M');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nburn = steps//2\n",
    "plt.plot(chain[Nburn:,0], chain[Nburn:,1], '.', alpha=0.1, label='MCMC samples')\n",
    "plt.xlabel('B')\n",
    "plt.ylabel('M');\n",
    "plt.plot(R.x[0] + dbm[:,0], R.x[1] + dbm[:,1], 'k-', label='Constraints from optimize()');\n",
    "#plt.plot(R.x[0] + 2.*dbm[:,0], R.x[1] + 2.*dbm[:,1], 'r-');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MCMC algorithm offers beautiful theoretical properties, but only in the limit of *infinite samples*.\n",
    "\n",
    "*In the limit*, the distribution of the chain returned by MCMC matches the distribution of the *logprob* function\n",
    "you gave it.\n",
    "\n",
    "This is bitter sweet, and why using MCMC retains some mystique.\n",
    "\n",
    "Some things that cause problems:\n",
    "\n",
    "- multi-modal distributions -- mountains separated by prairies\n",
    "- step sizes too big -- big jumps, often rejected\n",
    "- step sizes too small -- timid jumps, too often accepted\n",
    "\n",
    "And an over-arching question is \"is my chain converged yet?\"  That is, would my result change if I ran for twice as long?  There is \"folk wisdom\" but no objectively satisfying answer here.\n",
    "\n",
    "Brief warnings (\"here by dragons\").  If you go here, you need to read about *Detailed Balance* to ensure your\n",
    "algorithm still works:\n",
    "\n",
    "- beware non-symmetric jumps (eg anything other than a Gaussian jump)\n",
    "- don't try to adjust step sizes as the algorithm proceeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[show more DFM slides]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Step sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our MCMC routine above, we jump in all parameters at once.  Instead, we can jump in just parameter per round,\n",
    "and record the acceptance rates per parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_two(logprob, p0, stepsizes, steps, logprob_args):\n",
    "    '''\n",
    "    * logprob: a function that returns the log-probability at a given value of parameters.\n",
    "        lnp = logprob(p, *logprob_args)\n",
    "    * p0: initial position in parameter space (np array, size P)\n",
    "    * stepsizes: (np array, size P): size of Gaussian jumps in parameter space\n",
    "    * step: int: number of MCMC steps to take\n",
    "    * logprob_args: extra arguments passed to the *logprob* function\n",
    "    \n",
    "    Returns  (chain, logprobs, naccept)\n",
    "    * chain: size Nsteps x P, MCMC samples\n",
    "    * logprobs: size Nsteps, logprobs at *chain* positions\n",
    "    * njump: size P: number of MCMC proposed jumps for each parameter\n",
    "    * naccept: size P: number of MCMC proposed jumps that were accepted for each parameter\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial position in parameter space\n",
    "p0 = \n",
    "# \"Jump\" sizes\n",
    "stepsizes = \n",
    "# Number of MCMC steps\n",
    "steps = 5000\n",
    "chain, logprobs, njump, naccept = mcmc_two(log_likelihood_one, p0, stepsizes, steps, (x, y, yerr))\n",
    "print(naccept / njump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we might handle our data set with outliers using a probabilistic modeling approach.\n",
    "\n",
    "To do this, we're going to use a *mixture model*.  This is a kind of model where we say that each data point\n",
    "can be drawn from one of several different distributions.  We don't know which distribution each data point was\n",
    "drawn from, of course!\n",
    "\n",
    "A mixture model is straightforward to write down.  Say we have two distributions (or \"components\"), $A$ and $B$,\n",
    "with parameters $PA$ and $PB$, and a *mixing weight* $\\alpha$.  Then we can write the likelihood as\n",
    "\n",
    "$p(x | \\alpha, PA, PB) = \\alpha \\, p_A(x | PA) + (1 - \\alpha) p_B(x | PB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the $p(...)$ syntax is really strongly overloaded -- I added the subscript $A$ on $p_A$ to remind us\n",
    "that this is the probability distribution for mixture component $A$.  This could have a totally different functional form than $p_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we *believe* the data are generated by *either* process $A$ or process $B$, but when we're *inferring* the parameters of $A$ and $B$, we don't know which process generated each data point, so we have to *sum* (marginalize) over the possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an outlier model, our two distributions are going to be the \"foreground\" or \"good\" process -- the straight line model with errors $yerr$ -- and the \"background\" or \"bad\" process, which generates outliers.  What distribution should we write down for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple approach is to take the range of your data and say that the \"outlier\" distribution has equal likelihood over that whole range.  So, we might give it a Uniform distribution over the range `0` to `800` (being a bit generous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I'm going to rename $\\alpha$ to `p_good` -- the probability that a data point is good.  And we're going to infer that `p_good`.  If we had a more complicated background model, we could infer its parameters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_two(params, x, y, yerr, ylo, yhi):\n",
    "    p_good, b, m = params\n",
    "    \n",
    "    # p_good valid?\n",
    "    \n",
    "    ypred =\n",
    "    chi = \n",
    "    loglike_fg = \n",
    "    loglike_bg = \n",
    "    loglike = np.logaddexp()\n",
    "    return np.sum(loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what the *likelihood* for $y$ looks like for a single $x$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = 300\n",
    "yei = 40\n",
    "yy = np.linspace(0, 800, 500)\n",
    "llAi = [log_likelihood_two((1.0, 200., 1.), xi, yi, yei, 0., 800.) for yi in yy]\n",
    "llBi = [log_likelihood_two((0.0, 200., 1.), xi, yi, yei, 0., 800.) for yi in yy]\n",
    "plt.plot(yy, np.exp(llAi), 'g-', alpha=0.5, lw=2, label='Foreground model')\n",
    "plt.plot(yy, np.exp(llBi), 'r-', alpha=0.5, lw=2, label='Background model')\n",
    "lli = [log_likelihood_two((0.5, 200., 1.), xi, yi, yei, 0., 800.) for yi in yy]\n",
    "plt.plot(yy, np.exp(lli), 'b-', label='Mixture model')\n",
    "plt.xlabel('Y value')\n",
    "plt.ylabel('Likelihood for one data point')\n",
    "plt.legend()\n",
    "plt.axhline(0., color='k', alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pin `p_good` at a constant and see what the whole `B,M` parameter space looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB,MM = np.meshgrid(np.linspace(100., 300., 100), np.linspace(0.5, 2.5, 100))\n",
    "LL = np.zeros_like(BB)\n",
    "for i,(bb,mm) in enumerate(zip(BB.ravel(), MM.ravel())):\n",
    "    LL.flat[i] = log_likelihood_two((0.8, bb, mm), x, y, yerr, 0., 800.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(LL, origin='lower', extent=[BB.min(), BB.max(), MM.min(), MM.max()], aspect='auto')\n",
    "plt.xlabel('B')\n",
    "plt.ylabel('M')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = [0.8, 100., 2.]\n",
    "stepsizes = [0.01, 10., 0.01]\n",
    "steps = 10000\n",
    "chain, logprobs, naccept = mcmc_one(log_likelihood_two, p0, stepsizes, steps, (x, y, yerr, 0., 800.))\n",
    "naccept / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(chain[:,1], chain[:,2], 'b.', alpha=0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn = slice(steps//2,None)\n",
    "plt.plot(chain[burn,1], chain[burn,2], 'b.', alpha=0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "corner.corner(chain[burn,:], labels=['p_good', 'B', 'M'], fig=fig);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt='o', color='k');\n",
    "ax = plt.axis()\n",
    "xx = np.array(ax[:2])\n",
    "for i in range(20):\n",
    "    ii = np.random.randint(low=steps//2, high=steps)\n",
    "    pg,b,m = chain[ii,:]\n",
    "    plt.plot(xx, b + xx*m, 'b-', alpha=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
