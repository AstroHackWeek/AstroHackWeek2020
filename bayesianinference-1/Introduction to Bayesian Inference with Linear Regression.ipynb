{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Inference with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only necessary if you're running Python 2.7 or lower\n",
    "from __future__ import print_function, division\n",
    "from six.moves import range\n",
    "\n",
    "# import matplotlib and define our alias\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot figures within the notebook rather than externally\n",
    "%matplotlib inline\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy \n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although **linear regression** appears simple at first glance, it actually has a surprising amount of depth and is applicable in a bunch of different domains. Most importantly, it provides an accessible way to get a handle on several big concepts in **Bayesian Inference**, including:\n",
    "- defining objective functions,\n",
    "- probabilities and likelihoods, \n",
    "- estimating parameters,\n",
    "- the concept of priors, and\n",
    "- marginalizing over uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some mock data below. Don't worry about getting all the details here -- we will come back to them later as we explore how to fit a line better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mock_data(seed=123, m=0.875, b=2.523, s=0.523, unc=[0.2, 0.6], N=50):\n",
    "    \"\"\"\n",
    "    Generate `N` mock data points from the line\n",
    "    with slope `m`, intercept `b`, and\n",
    "    intrinsic scatter `s` with measurement uncertainties\n",
    "    uniformly distributed between the values in `unc` using\n",
    "    a random number generator with the starting `seed`.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rstate = np.random.RandomState(seed)\n",
    "    \n",
    "    # generate synthetic data\n",
    "    x = np.sort(3. * rstate.rand(N))  # x values\n",
    "    y_int = m * x + b  # underlying line\n",
    "    y = rstate.normal(y_int, s)  # with intrinsic scatter\n",
    "    yerr = rstate.uniform(unc[0], unc[1], N)  # measurement errors\n",
    "    yobs = rstate.normal(y, yerr)\n",
    "    \n",
    "    return x, yobs, yerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mock data\n",
    "x, y, ye = gen_mock_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', capsize=0)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our starting goal here is going to be pretty simple: **try to determine a simple linear relationship**. In other words, we want a model like\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "where $m$ is the slope and $b$ is the $y$-intercept. We will slowly make this model more complicated as we try and model additional features of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Chi by Eye\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's ignore all attempts to make this quantitative. The human brain is actually quite good at fitting models to data, so we can do a **\"chi by eye\"** process (based on the \"chi-squared\" statistic we will define later) and just try and benchmark what looks like a reasonable fit. \n",
    "\n",
    "**In the cell below, just take a minute or two to see if you can find a \"best guess\" for the slope and intercept.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimates of slope and intercept\n",
    "m_1, b_1 = ..., ...\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.7, color='red',\n",
    "         label='by eye')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your guess should look \"reasonable\", but you might not feel so good about it. Why? Can you say *why* it's the best fit? How about uncertainties on the slope and intercept?\n",
    "\n",
    "Part of the difficulty here is that there is no quantitative metric for what makes a fit \"good\". We will now try to rectify this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective (Loss) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to define whether a fit is any good is to see how well it actually *predicts* the data it is supposed to be fitting. One way to quantify this involves looking at the residuals\n",
    "\n",
    "$$ |\\Delta y_i| = |y_{i, {\\rm pred}} - y_{i}| $$\n",
    "\n",
    "where $i$ indicates the $i$th datapoint, $y_{i, {\\rm pred}} = m x_i + b$ is the prediction from the model and $y_{i}$ is the observed (noisy) value. We are interested in the absolute value because what probably matters is the total error, not whether it's positive or negative. Note that we are currently ignoring the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would ideally like to have a model that has the smallest residuals for all the points. Let's therefore define a **loss function**:\n",
    "\n",
    "$$ L(m, b | \\{ (x_1, y_1), \\dots, (x_N, y_N) \\}) = \\sum_{i=1}^{N} |\\Delta y_i|^p $$\n",
    "\n",
    "where the $|$ line now indicates \"given\" or \"conditioned on\". In other words, what is the loss of a particular slope $m$ and intercept $b$ given the data $\\{ (x_1, y_1), \\dots, (x_N, y_N) \\}$?\n",
    "\n",
    "$p$ is a power that all residuals are raised to that control how sensitive the loss function is to large and small residuals. Common values for $p$ are often $1 \\leq p \\leq 2$, with $p=2$ (squaring the residuals) being the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling in the loss function defined below, see how changing the values of the slope and intercept change the loss. Then experiment with how different values of $p$ change the behavior you see.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss(theta, p=2., x=x, y=y):\n",
    "    \"\"\"A simple loss function as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change m at fixed b\n",
    "mgrid = ...\n",
    "loss_m = np.array([loss([m, b_1], p=2.) for m in mgrid])\n",
    "    \n",
    "# change b at fixed m\n",
    "bgrid = ...\n",
    "loss_b = np.array([loss([m_1, b], p=2.) for b in bgrid])\n",
    "\n",
    "\n",
    "# plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mgrid, loss_m, lw=3, color='firebrick')\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('L')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bgrid, loss_b, lw=3, color='navy')\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('L')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best fit, we want to minimize our loss function. \n",
    "\n",
    "**With help from the [documentation](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.optimize.minimize.html), use the `minimize` function from `scipy.optimize` to get the best-fit parameters `theta` based on our loss function `loss`. Quantify the improvement between your initial guess and this new result based on the loss function values. Are they significant? Why or why not?**\n",
    "\n",
    "If you have extra time, explore how these change as you vary $p$ away from the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize the loss function\n",
    "results = minimize(loss, ...)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_2, b_2 = results['x']\n",
    "\n",
    "# plot it against previous results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.7, color='dodgerblue',\n",
    "         label='loss')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Errors with Chi-Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we haven't tried to incorporate **errors** into our linear regression model. We probably want the fit to take into account whether a point has large or small errors. Points with large errors probably should be \"down-weighted\" in the fit since they are more uncertain. We can accomplish this be defining the error-normalized resuidual $\\chi$:\n",
    "\n",
    "$$ \\chi_i = \\left|\\frac{\\Delta y_i}{\\sigma_i}\\right| $$\n",
    "\n",
    "where $\\sigma_i$ is the measurement error associated with $y_i$. This should make some intuitive sense: we are just normalizing the residual by the error, so now we are measuring how discrepant the prediction is in units of the error bar for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the **chi-square statistic** often used in the astronomical literature:\n",
    "\n",
    "$$ \\chi^2(m, b | \\{ (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N) \\}) = \\sum_{i=1}^{N} \\chi^2_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following the example above, define a chi-square function and minimize it. Then quantify how much better the new best-fit model is compared to the previous two. Is this a substantial improvement over what we found earlier? Why or why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2 function\n",
    "def chi2(theta, x=x, y=y, ye=ye):\n",
    "    \"\"\"Chi-square as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize chi2\n",
    "results = minimize(chi2, ...)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_3, b_3 = results['x']\n",
    "\n",
    "# plot it against previous results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.7, color='darkviolet',\n",
    "         label='chi2')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to consider an additional source of uncertainty: some amount of **intrinsic scatter** in the fitted relationship. In other words, in addition to the scatter $\\sigma_i$ caused by the observational uncertainties, we also want to add on an additional scatter $s_i = s$ that is the same for all points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How might we do so? Well, maybe we could try something like\n",
    "\n",
    "$$\\chi = \\left|\\frac{\\Delta y_i}{\\sigma_i + s}\\right|$$\n",
    "\n",
    "and then write down our $\\chi^2$ model as before, but now as a function of $m$, $b$, and $s$. Notice, however, that $\\chi^2$ will always get smaller as $s$ increases, since that means the error-normalized residual gets smaller too. Whoops! Maybe we can add on a penalty to disfavor large values? But what would be an appropriate penalty? Maybe something like\n",
    "\n",
    "$$ \\chi^2(m, b, s | \\{ (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N) \\}) = s^2 + \\sum_{i=1}^{N} \\chi^2_i $$\n",
    "\n",
    "could work, and we could argue for reasons why it might be reasonable. This basic result, however, reveals a fundamental problem with how we're approached fitting a line so far: we haven't actually defined an underlying **model** for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models start with trying to understand the **data generating process**. In other words, if we wanted to simulate data based on an input set of parameters, how would we do so?\n",
    "\n",
    "**In the space below, write down the steps we would need to generate data with a given a slope $m$, intercept $b$, and scatter $s$ along with associated positions $x_i$ and uncertainties $\\sigma_i$ values.** Don't worry about the details -- just the large conceptual picture is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Space intentionally left blank*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, let's start with the observed $y$ values. We have observational errors, but what do those mean? Well, in general it means we assume the observed data follow a **Normal distribution** (also called a \"Gaussian\") such that the probability that $y_i$ is any particular value follows a **probability density function** of the form\n",
    "\n",
    "$$ P(y_i|y_{i,{\\rm true}}, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{1}{2}\\frac{(y_{i, {\\rm true}} - y_i)^2}{\\sigma_i^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model for the data is $y_{i, {\\rm true}} = m x_i + b$, this then gives\n",
    "\n",
    "$$ P(y_i|m, b, x_i, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left[-\\frac{1}{2}\\frac{(m x_i + b - y_i)^2}{\\sigma_i^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is beyond the scope of this notebook to prove, it can be shown that assuming that the intrinsic scatter $s$ is also Gaussian gives the modified result:\n",
    "\n",
    "$$ P(y_i|m, b, s, x_i, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi(\\sigma_i^2 + s^2)}} \\exp\\left[-\\frac{1}{2}\\frac{(m x_i + b - y_i)^2}{\\sigma_i^2 + s^2}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of independent events $A$, $B$, and $C$ all occuring is just the product of their individual probabilities:\n",
    "\n",
    "$$ P(A, B, C) = P(A) P(B) P(C) $$\n",
    "\n",
    "The same holds true for data points: if each data point is an independent observation, the total probability is just the individual probabilities for each data point multiplied together. This gives\n",
    "\n",
    "$$ P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = \\prod_{i=1}^{N} P(y_i|m, b, s, x_i, \\sigma_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the probabilities can get really small really fast, for numerical stability this is almost always re-written in logarithmic form:\n",
    "\n",
    "$$ \\ln P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = \\sum_{i=1}^{N} \\ln P(y_i|m, b, s, x_i, \\sigma_i) $$\n",
    "\n",
    "Notice that, if we explicitly substitute in the Gaussian probability density function, we get this intriguing result:\n",
    "\n",
    "$$ \\ln P(y_1, \\dots, y_N|m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) = -\\frac{1}{2} \\sum_{i=1}^{N} \\frac{(mx_i + b - y_i)^2}{\\sigma_i^2 + s^2} + \\ln(2\\pi(\\sigma_i^2 + s^2)) $$\n",
    "\n",
    "The first term here looks a lot like our original $\\chi^2$ expression, except now with the modified errors. And the second term now looks a lot like a penalty term that disfavors larger $s$ values! So we can see that by explicitly writing out a model, we naturally accomplish our original objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-Likelihood Estimate (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"best-fit\" parameters now can be defined as those that **maximize the likelihood** (or, equivalently, minimize the negative of the likelihood).\n",
    "\n",
    "**Write down a function that computes the (negative) log-likelihood defined above and minnimize it. Quantify the difference between these maximum-likelihood results and those derived earlier from the best-fit $\\chi^2$. Does the model favor adding in this additional scatter? Why or why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negloglike(theta, x=x, y=y, ye=ye):\n",
    "    \"\"\"(Negative) log-likelihood as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b, s = theta  # reassign parameters\n",
    "    ypred = m * x + b\n",
    "    resid = ...\n",
    "    logl = ...\n",
    "    \n",
    "    return -logl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize negative log-likelihood\n",
    "results = minimize(negloglike, ...)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_4, b_4, s_4 = results['x']\n",
    "\n",
    "# plot it against previous results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.4, color='darkviolet',\n",
    "         label='chi2')\n",
    "plt.plot(x, m_4 * x + b_4, lw=4, alpha=0.7, color='darkorange', \n",
    "         label='MLE')\n",
    "[plt.fill_between(x, m_4 * x + b_4 + s_4 * n, m_4 * x + b_4 - s_4 * n, \n",
    "                  alpha=0.02, color='darkorange')\n",
    " for n in np.linspace(0, 2, 20)]\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a working model that can describe the underlying probabilistic processes that generate the data. However, we're still missing one small thing. Notice that, in our earlier notation,\n",
    "\n",
    "$$ P(A | B) $$\n",
    "\n",
    "describes the probability of $A$ *given* (i.e. conditioned on) $B$. So our likelihood\n",
    "\n",
    "$$ P(y_1, \\dots, y_N | m, b, s, (x_1, \\sigma_1), \\dots, (x_N, \\sigma_N)) $$\n",
    "\n",
    "describes something similar: the probability of seeing the observed data $\\{ y_1, \\dots, y_N\\}$ *given* the underlying model parameters $m$, $b$, and $s$ along with the $x_i$ and $\\sigma_i$ values. What we *want*, however, is this expression:\n",
    "\n",
    "$$ P(m, b, s | (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N)) $$\n",
    "\n",
    "This now describes the probability of our model parameters given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting at this distribution requires exploiting the fact that probabilities can factor. This allows us to rewrite:\n",
    "\n",
    "$$ P(A, B) = P(A|B)P(B) = P(B|A)P(A) = P(B, A) \\:\\:\\:\\Rightarrow\\:\\:\\: P(B|A) = \\frac{P(A|B) P(B)}{P(A)} $$\n",
    "\n",
    "If we replace $A = {\\rm data}$ and $B = {\\rm parameters}$, we are left with what's known as **Bayes' Theorem**:\n",
    "\n",
    "$$ P({\\rm parameters} \\,|\\, {\\rm data}) = \\frac{P({\\rm data}\\,|\\,{\\rm parameters}) \\, P({\\rm parameters})}{P({\\rm data})} $$\n",
    "\n",
    "Pulling apart this expression:\n",
    "- The term $P({\\rm data}\\,|\\,{\\rm parameters})$, is the **likelihood** that we have been working with already and describes the probability of seeing the data given the model. \n",
    "- The term $P({\\rm parameters})$ is known as the **prior**: it characterizes our prior knowledge about the parameters is question without seeing the data. \n",
    "- The term $P({\\rm data})$ is known as the **evidence** (or marginal likelihood). Since this is just a constant, most often we can just ignore it.\n",
    "- Finally, the left-hand side $P({\\rm parameters} \\,|\\, {\\rm data})$ is known as the **posterior** since it combines the prior and the likelihood together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central component of Bayesian inference is the prior, which describes our prior knowledge of the parameters. This is where all of the power (and drawbacks) of Bayesian inference comes from, since the priors can dramatically affect the analysis.\n",
    "\n",
    "As an example, consider priors on the components that are independent and Gaussian, analagous to our data. If we wrote out the new log-posterior, we would then get:\n",
    "\n",
    "$$ \\ln P(m, b, s | (x_1, y_1, \\sigma_1), \\dots, (x_N, y_N, \\sigma_N)) = \\left\\{-\\frac{1}{2} \\sum_{i=1}^{N} \\frac{(mx_i + b - y_i)^2}{\\sigma_i^2 + s^2} + \\ln(2\\pi(\\sigma_i^2 + s^2)) \\right\\} + \\left\\{-\\frac{1}{2} \\sum_{j=\\{m,b,\\sigma\\}} \\frac{(j - \\mu_j)^2}{\\sigma_j^2} + \\ln(2\\pi\\sigma_j^2)\\right\\} + C $$\n",
    "\n",
    "where $C$ is again a constant that we can ignore. We can see the priors correspond to additional penalty terms on the parameters that function like additional data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Maximum-a-Posteriori* (MAP) Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now find our parameter estimates with the highest log-posterior values, known as the **maximum-a-posteriori** (MAP) estimate.\n",
    "\n",
    "**Using the functional form above, define a Gaussian log-prior over each of the three parameters by specifying a given mean and standard deviation. Then, minimize the (negative) log-posterior.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_means = np.array([1., 3., 0.5])  # m, x, b\n",
    "prior_stds = np.array([0.25, 0.5, 0.15])  # m, x, b\n",
    "\n",
    "def neglogprior(theta, mean=prior_means, std=prior_stds):\n",
    "    \"\"\"(Negative) log-prior as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def neglogpost(theta, x=x, y=y, ye=ye, mean=prior_means, std=prior_stds):\n",
    "    \"\"\"(Negative) log-posterior as a function of parameters `theta`.\"\"\"\n",
    "    \n",
    "    m, b, s = theta  # reassign parameters\n",
    "    logp = -neglogprior(theta, mean=mean, std=std)\n",
    "    logl = -negloglike(theta, x=x, y=y, ye=ye)\n",
    "    \n",
    "    return -(logl + logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimize negative log-likelihood\n",
    "results = minimize(neglogpost, ...)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best-fit result\n",
    "m_5, b_5, s_5 = results['x']\n",
    "\n",
    "# plot it against previous results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(x, y, yerr=ye, fmt=\"ko\", ecolor='gray', \n",
    "             capsize=0, label='Data')\n",
    "plt.plot(x, m_1 * x + b_1, lw=4, alpha=0.4, color='red',\n",
    "         label='by eye')\n",
    "plt.plot(x, m_2 * x + b_2, lw=4, alpha=0.4, color='dodgerblue',\n",
    "         label='loss')\n",
    "plt.plot(x, m_3 * x + b_3, lw=4, alpha=0.4, color='darkviolet',\n",
    "         label='chi2')\n",
    "plt.plot(x, m_4 * x + b_4, lw=4, alpha=0.4, color='darkorange', \n",
    "         label='MLE')\n",
    "plt.plot(x, m_5 * x + b_5, lw=4, alpha=0.7, color='gray', \n",
    "         label='MAP')\n",
    "[plt.fill_between(x, m_5 * x + b_5 + s_5 * n, m_5 * x + b_5 - s_5 * n, \n",
    "                  alpha=0.02, color='gray')\n",
    " for n in np.linspace(0, 2, 20)]\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to just providing a best-fit solution, the posterior distribution now quantifies just how much more likely one model is than another. While exploring this fully is beyond the scope of this introduction, one neat result of using a properly defined probability as the function to be minimize is that the minimization routine provides estimates of the parameter errors via the `hess_inv` item in the output results dictionary. This allows us to visualize how our final posterior distribution compares to our input priors, which is shown below.\n",
    "\n",
    "**Using the plots below, explore how changing the priors impacts the inferred solution. At what point do the best-fit values and errors become \"prior-dominated\" (prior largely determines the results)? At what point do they become \"likelihood-dominated\" (prior has little impact on the result)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = int(1e6)\n",
    "\n",
    "# generate prior samples\n",
    "m_prior, b_prior, s_prior = np.random.multivariate_normal(prior_means, \n",
    "                                                          np.diag(prior_stds**2), \n",
    "                                                          size=n).T\n",
    "\n",
    "# generate posterior samples\n",
    "m_post, b_post, s_post = np.random.multivariate_normal(results['x'], \n",
    "                                                       results['hess_inv'], \n",
    "                                                       size=n).T\n",
    "\n",
    "# generate 1-D histograms of priors vs posteriors\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(m_prior, 100, density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(m_post, 100, density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "plt.legend()\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(b_prior, 100, density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(b_post, 100, density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(s_prior, 100, density=True, label='prior',\n",
    "         color='dodgerblue', alpha=0.6)\n",
    "plt.hist(s_post, 100, density=True, label='post',\n",
    "         color='firebrick', alpha=0.6)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('Probability')\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "\n",
    "# generate 2-D histograms of posteriors\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist2d(m_post, b_post, 100,\n",
    "           cmap='Reds', alpha=0.6)\n",
    "plt.hist2d(m_prior, b_prior, 100,\n",
    "           cmap='Blues', alpha=0.6)\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('b')\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist2d(b_post, s_post, 100,\n",
    "           cmap='Reds', alpha=0.6)\n",
    "plt.hist2d(b_prior, s_prior, 100,\n",
    "           cmap='Blues', alpha=0.6)\n",
    "plt.xlabel('b')\n",
    "plt.ylabel('s')\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist2d(s_post, m_post, 100,\n",
    "           cmap='Reds', alpha=0.6)\n",
    "plt.hist2d(s_prior, m_prior, 100,\n",
    "           cmap='Blues', alpha=0.6)\n",
    "plt.xlabel('s')\n",
    "plt.ylabel('m')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional way to view the results is to generate the **posterior predictive**. This just means to actually see what the different fits look like overplotted on top of the data (i.e. the predictions *marginalized over* the parameter uncertainties). **If you have extra time, try visualizing this uncertainty by overplotting the parameters generated above over the data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot posterior predictive"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
